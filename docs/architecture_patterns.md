  
**Khorsyio Framework**

Анализ применимости и архитектурные паттерны

Версия 2.0  |  2026

# **1\. Ключевые концепции фреймворка**

Khorsyio \- async Python фреймворк с событийно-ориентированной архитектурой. Бизнес-логика реализуется через изолированные блоки (handlers), которые взаимодействуют исключительно через типизированные события на шине. Прямые вызовы между блоками отсутствуют. Стек: asyncio, msgspec (сериализация), asyncpg (postgres), socketio (websocket), ASGI (http).

| Концепция | Описание |
| :---- | :---- |
| Handler | Единица бизнес-логики. Один вход (subscribes\_to), один выход (publishes), изолированное состояние. Реализует единственный метод process(data, ctx) |
| Struct (msgspec) | Типизированный контракт данных между блоками. Является документацией, валидацией и сериализацией одновременно. Быстрее pydantic в 10-50 раз |
| Событие | Строка вида namespace.action (например order.validate). Шина маршрутизирует по подпискам автоматически |
| Context (ctx) | Сквозной контекст запроса: trace\_id, user\_id, extra (dict). Автоматически прокидывается через всю цепочку при forward между handlers |
| Domain | Группировка handlers и routes с единым namespace. При namespace='order' событие 'validate' становится 'order.validate' |
| Bus | Центральная шина. Маршрутизация, bus.request (publish \+ wait), метрики per-handler, event log, scheduled tasks, graceful shutdown с drain |
| Error | Стандартная структура ошибки: code, message, source, trace\_id, details. Envelope.is\_error для проверки |
| DI | Инъекция зависимостей через имя параметра в \_\_init\_\_: db, client, bus, transport, app. Без декораторов и конфигурации |

## **Анатомия блока**

Каждый handler определяет четыре атрибута и реализует единственный метод process. Десериализация входа, сериализация выхода, создание envelope и прокидывание context выполняются фреймворком автоматически.

class PriceHandler(Handler):

    subscribes\_to \= 'validated'      \# входное событие

    publishes \= 'priced'             \# выходное событие

    input\_type \= OrderState           \# struct на входе

    output\_type \= OrderState          \# struct на выходе

    timeout \= 5.0                     \# таймаут обработки

    async def process(self, data: OrderState, ctx: Context) \-\> OrderState:

        data.unit\_price \= CATALOG\[data.product\]

        data.total \= data.unit\_price \* data.quantity

        data.status \= 'priced'

        return data

## **Инъекция зависимостей**

Имя параметра в \_\_init\_\_ определяет что инжектится. Фреймворк автоматически передает нужный компонент при создании handler через Domain. Маппинг: db \-\> app.db (asyncpg pool), client \-\> app.client (httpx session), bus \-\> app.bus, transport \-\> app.transport (socketio), app \-\> весь объект приложения. Можно комбинировать несколько зависимостей.

class FetchHandler(Handler):

    def \_\_init\_\_(self, db, client):     \# получит app.db и app.client

        self.\_db \= db

        self.\_client \= client

## **Два паттерна структур данных**

**Pipeline (единый state).** Один struct обогащается по мере прохождения цепочки. Каждый блок дописывает свои поля, не трогая чужие. Удобно для трассировки: в финальном объекте видно весь путь. Используется в линейных бизнес-процессах (заказ, заявка, airdrop).

**Transform (вход/выход).** Каждый блок имеет свой input и output struct. Вход и выход существенно отличаются по природе. Блоки более переиспользуемы. Используется в ETL, report pipeline, data processing.

# **2\. Режимы работы фреймворка**

Фреймворк поддерживает пять режимов организации потока данных. Режимы комбинируются: один процесс может начинаться как http-запрос (режим 1), содержать условную маршрутизацию (режим 5\) и завершаться fan-out уведомлениями (режим 4).

## **Режим 1\. HTTP \-\> цепочка \-\> ответ клиенту**

HTTP endpoint запускает цепочку через bus.request и синхронно ждет финальный ответ с совпадающим trace\_id. Клиент получает результат обработки всей цепочки в одном http response. Подходит для REST API где клиент ожидает результат.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | POST /order | Клиент отправляет запрос с json body |
| 2 | order.validate | Проверка полей, нормализация данных |
| 3 | order.priced | Расчет цены по каталогу |
| 4 | order.discounted | Применение скидки лояльности по customer\_id |
| 5 | order.reserved | Проверка остатков, резервирование на складе |
| 6 | order.paid | Списание средств, генерация payment reference |
| 7 | order.confirmed | Формирование order\_id, финальный статус |
| 8 | HTTP 201 | bus.request получает ответ, Response.json клиенту |

**Ключевой механизм:** bus.request публикует первое событие цепочки и создает waiter по ключу response\_type \+ trace\_id. Когда финальный handler публикует event с тем же trace\_id, waiter срабатывает и возвращает envelope вызывающему коду. При таймауте возвращается error envelope с code='timeout'.

result \= await bus.request('order.validate', data,

    response\_type='order.confirmed', source='http', timeout=10.0)

## **Режим 2\. Worker / Fire-and-forget**

Scheduled task или публикация без ожидания. Цепочка работает автономно, результат пишется в базу, лог или внешний сервис. Нет bus.request, нет http, нет ожидания ответа.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | schedule / publish | Планировщик или внешний триггер запускает цепочку |
| 2 | report.collect | Сбор метрик из нескольких источников |
| 3 | report.aggregated | Агрегация: count, total, average, max, min |
| 4 | report.enriched | Добавление метаданных: период, версия, source |
| 5 | report.formatted | Форматирование в текстовый отчет |
| 6 | report.delivered | Отправка: лог, webhook, email, файл в S3 |

app.bus.schedule('report.collect', ReportTrigger(), interval=3600.0)

await bus.publish('report.collect', ReportTrigger(), source='manual')

## **Режим 3\. WebSocket \-\> цепочка \-\> ответ отправителю**

WebSocket клиент отправляет событие. Цепочка обрабатывает данные. Финальный handler возвращает ответ конкретному клиенту через transport.reply\_to\_sender(envelope). sid клиента сохраняется в ctx.extra\['\_ws\_sid'\] автоматически и прокидывается через всю цепочку.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | ws: auction.bid | Участник отправляет ставку через WebSocket |
| 2 | bid.validate | Проверка: лот активен, сумма выше текущей, баланс достаточен |
| 3 | bid.lock | Оптимистичная блокировка для конкурентных ставок |
| 4 | bid.persist | Запись ставки в БД, обновление текущего лидера |
| 5 | bid.notify\_all | Fan-out: broadcast всем подключенным через transport.emit |
| 6 | bid.reply\_sender | Ответ инициатору через transport.reply\_to\_sender |

## **Режим 4\. Fan-out (параллельные подписчики)**

Несколько handlers подписаны на одно событие. Все получают копию данных и работают параллельно через asyncio.gather. Отказ одного не блокирует остальных. Добавление нового обработчика не требует изменения существующих блоков и не требует деплоя других частей системы.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | order.confirmed | Стартовая точка fan-out (финальное событие заказа) |
| 2 | SendEmailHandler | Отправка подтверждения клиенту |
| 3 | UpdateInventoryHandler | Списание товаров с остатков склада |
| 4 | CreateInvoiceHandler | Генерация счета |
| 5 | TrackAnalyticsHandler | Запись метрик |
| 6 | NotifySlackHandler | Уведомление команды |

**Почему это работает:** в классическом подходе добавление нового side-effect требует найти место в коде и вставить вызов, рискуя сломать existing flow. В событийной модели новый handler просто подписывается на событие. Существующий код не меняется. Тестирование изолированное.

## **Режим 5\. Условная маршрутизация**

Handler решает на какое событие публиковать результат, возвращая Envelope напрямую. publishes='' (пустой). Позволяет реализовать ветвление потока без внешнего оркестратора.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | payment.process | RouterHandler получает платеж |
| 2 | amount \> 10000 | payment.manual\_review (ручная проверка) |
| 3 | currency \!= EUR | payment.convert (конвертация валюты) |
| 4 | 3ds required | payment.verify\_3ds (подтверждение) |
| 5 | standard path | payment.execute (исполнение) |

async def process(self, data, ctx):

    if data.amount \> 10000:

        return Envelope.create('payment.manual\_review', data,

            source='RouterHandler', trace\_id=ctx.trace\_id)

**Важно:** при создании Envelope вручную передавай trace\_id из ctx чтобы не потерять сквозную трассировку. Или используй envelope.forward() который делает это автоматически.

# **3\. Комплексные сценарии применения**

Развернутые кейсы где событийная блочная архитектура дает наибольшее преимущество. Для каждого показано: flow, почему khorsyio выигрывает, и что было бы сложнее без фреймворка.

## **Сценарий А. Airdrop-кампания с multi-chain распределением**

Типичная задача в блокчейн-проектах: принять заявку, проверить eligibility, определить сеть, подписать транзакцию, отправить и отслеживать статус. Каждый этап изолирован и имеет четкий контракт.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | airdrop.submit | Принять заявку: адрес кошелька, network, campaign\_id |
| 2 | airdrop.verify | Проверить eligibility: whitelist, баланс, предыдущие клеймы |
| 3 | airdrop.route | Условная маршрутизация: base / polygon / bsc / arbitrum |
| 4 | airdrop.sign | Генерация EIP-712 подписи для конкретной сети |
| 5 | airdrop.broadcast | Отправка транзакции через RPC провайдер |
| 6 | airdrop.monitor | Мониторинг статуса: pending \-\> confirmed / failed |
| 7 | airdrop.complete | Fan-out: обновить БД \+ уведомить пользователя \+ аналитика |

### **Почему khorsyio здесь выигрывает**

**Изоляция RPC-вызовов.** Каждая сеть (base, polygon, bsc) реализуется отдельным handler с собственным timeout и error handling. Ошибка RPC одной сети не затрагивает обработку других. В монолитном коде это обычно один большой switch-case с try-except, где ошибка в одной ветке может заблокировать весь flow.

**Условная маршрутизация на шаге route.** Handler airdrop.route анализирует network из заявки и публикует событие в нужную ветку: airdrop.sign.base, airdrop.sign.polygon и т.д. Каждая ветка может иметь свою логику подписи. Добавление новой сети \- это новый handler, не правка существующего кода.

**Fan-out на финале.** После подтверждения транзакции три действия выполняются параллельно: обновление БД, отправка уведомления, запись аналитики. Если email-сервис лежит, балансы в БД все равно обновятся. В синхронном коде падение email-отправки часто ломает всю обработку.

**Трассировка.** Единый trace\_id проходит через всю цепочку от submit до complete. При проблеме с конкретным airdrop можно запросить event\_log по trace\_id и увидеть на каком именно шаге и с какой ошибкой произошел сбой. В обычном коде для этого нужно вручную пробрасывать correlation\_id через каждый вызов.

## **Сценарий Б. Real-time аукцион с WebSocket**

Аукционная платформа со ставками в реальном времени. Требует синхронного ответа участнику и параллельного уведомления всех наблюдателей. Комбинация режимов 3, 4 и 2\.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | ws: auction.bid | Участник отправляет ставку через WebSocket |
| 2 | bid.validate | Проверить: лот активен, сумма выше текущей, баланс достаточен |
| 3 | bid.lock | Оптимистичная блокировка для конкурентных ставок |
| 4 | bid.persist | Запись ставки в БД, обновление текущего лидера |
| 5 | bid.notify\_all | Fan-out: broadcast всем через transport.emit |
| 6 | bid.reply\_sender | Ответ инициатору через transport.reply\_to\_sender |
| 7 | bid.check\_end | Worker: фоновая проверка не истекло ли время аукциона |

### **Почему khorsyio здесь выигрывает**

**Естественное соответствие WebSocket-модели.** Событие от ws клиента сразу попадает на шину как envelope с ctx.extra\['\_ws\_sid'\]. Это не адаптация REST к realtime, а нативный event-driven подход. Шина обрабатывает ws-события точно так же как http-события, с теми же handlers, метриками и трассировкой.

**Разделение ответа и уведомления.** bid.reply\_sender отправляет персональный ответ конкретному участнику (принята/отклонена). bid.notify\_all рассылает обновление всем наблюдателям. Это два разных handler, два разных concern. В типичном ws-коде оба действия обычно в одной функции, и добавление логики уведомления усложняет обработку ставки.

**Конкурентные ставки.** bid.lock реализует оптимистичную блокировку как изолированный блок. Если две ставки приходят одновременно, шина обрабатывает их последовательно (asyncio single-threaded), lock handler проверяет текущую цену в БД и отклоняет проигравшую. Логика блокировки отделена от логики валидации и записи.

## **Сценарий В. Система заявок (tickets) с автоматической классификацией**

Реальный пример из demo: заявка проходит через 4 блока, каждый обогащает единый TicketState. Классификация по ключевым словам, назначение ответственного, сохранение в postgres.

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | ticket.validate | Проверка: title \>= 3 символов, author \>= 2\. Нормализация |
| 2 | ticket.classified | Анализ текста: определение category и priority по ключевым словам |
| 3 | ticket.assigned | Назначение команды по category: billing-\>finance, technical-\>engineering |
| 4 | ticket.created | INSERT в postgres, возврат id и timestamp |

### **Почему khorsyio здесь выигрывает**

**Замена одного блока без последствий.** ClassifyHandler сейчас использует keyword matching. Завтра его можно заменить на ML-классификатор. Контракт не меняется: на входе TicketState со status='validated', на выходе TicketState с category и priority. Остальные блоки не узнают о замене.

**Расширение без переделки.** Нужно добавить SLA-расчет? Новый handler ticket.sla\_calculated между assigned и created. Нужно уведомлять в Slack при critical? Новый handler подписанный на ticket.created с проверкой priority. Ни одна строка существующего кода не меняется.

**Единый TicketState как audit trail.** Поле steps собирает действия каждого блока: \['validate:ok', 'classify:technical/critical', 'assign:engineering\_team', 'persist:id=42'\]. По одному ответу видно весь путь заявки. В классическом подходе для аналогичного audit trail нужен отдельный механизм логирования.

## **Сценарий Г. DeFi: обработка swap-транзакции**

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | swap.init | Получить параметры: token\_in, token\_out, amount, slippage |
| 2 | swap.quote | Запросить котировку через внешний API агрегатора |
| 3 | swap.validate\_slippage | Проверить slippage в допустимом диапазоне |
| 4 | swap.approve\_check | Проверить approve токена для контракта |
| 5 | swap.build\_tx | Построить транзакцию с gas параметрами |
| 6 | swap.sign\_submit | Подписать и отправить в сеть |
| 7 | swap.monitor | Worker: опрос статуса каждые N секунд |
| 8 | swap.finalize | Fan-out: обновить балансы \+ история \+ уведомление |

### **Почему khorsyio здесь выигрывает**

**Точная локализация ошибок.** Swap-процесс имеет множество точек отказа: API агрегатора недоступен (swap.quote), slippage превышен (swap.validate\_slippage), газ вырос (swap.build\_tx), транзакция отклонена (swap.sign\_submit). Каждая ошибка локализована в конкретном handler с конкретным error.code и error.source. В монолитной функции из 200 строк ошибка 'transaction failed' не говорит на каком именно этапе произошел сбой.

**Timeout per handler.** swap.quote имеет timeout=5s (внешний API). swap.monitor имеет timeout=120s (ожидание подтверждения в сети). swap.validate\_slippage имеет timeout=1s (чистый расчет). Каждый блок получает адекватный своей задаче timeout. В общем try-except один timeout для всего.

**Метрики per handler.** bus.metrics.snapshot() покажет что swap.quote в среднем занимает 800ms, а swap.build\_tx 50ms. Если среднее время swap.quote выросло с 800ms до 3s \- проблема в API агрегатора. Без per-handler метрик видно только общее время swap, и непонятно где bottleneck.

## **Сценарий Д. ETL-пайплайн с трансформацией данных**

| \# | Событие | Описание |
| :---- | :---- | :---- |
| 1 | etl.ingest | Прочитать сырые данные из источника (S3, API, БД) |
| 2 | etl.clean | Удалить null-записи, нормализовать типы, дедупликация |
| 3 | etl.validate | Проверить бизнес-правила, пометить некорректные записи |
| 4 | etl.enrich | Дополнить данными из справочников |
| 5 | etl.transform | Применить бизнес-трансформации, рассчитать агрегаты |
| 6 | etl.load | Загрузить в целевое хранилище |
| 7 | etl.report | Сформировать отчет: обработано, ошибок, пропущено |

Стратегия ошибок через status в struct: каждый блок проверяет статус предыдущего и при несовпадении прокидывает данные дальше без обработки. Финальный etl.report получает state с полной информацией о том что произошло на каждом этапе.

# **4\. Стратегии обработки ошибок**

Фреймворк предоставляет два подхода к ошибкам. Выбор определяет поведение всей цепочки.

## **Стратегия 1\. Status в struct (мягкий режим)**

Блок записывает статус ошибки в данные и возвращает их. Следующие блоки проверяют статус и пропускают обработку. Данные проходят всю цепочку до конца, собирая информацию по пути.

if not data.wallet\_address:

    data.status \= 'error: wallet\_address required'

    return data  \# дальше по цепочке без обработки

**Когда использовать:** частичные ошибки допустимы, нужно накапливать ошибки по нескольким полям, важно сохранить audit trail даже при невалидных данных, клиент должен получить подробный ответ о причине отказа.

## **Стратегия 2\. Exception (жесткий режим)**

Блок бросает исключение, шина перехватывает и создает error envelope с code='handler\_error'. Цепочка прерывается. bus.request получает error envelope.

if not await self.\_has\_permission(ctx.user\_id):

    raise PermissionError('access denied')

**Когда использовать:** ошибка критическая и дальнейшая обработка бессмысленна, при нарушении инварианта бизнес-логики, при системных ошибках (БД недоступна, внешний сервис не отвечает).

## **Комбинированная стратегия**

На практике в одной цепочке используются оба подхода: валидационные ошибки через status (пользователь не заполнил поле), системные ошибки через exception (RPC-соединение разорвано). bus.request корректно обрабатывает оба варианта: status-ошибка приходит в финальном envelope, exception приходит как error envelope с is\_error=True.

# **5\. Матрица применимости**

## **Высокая применимость**

| Сценарий | Обоснование | Оценка |
| :---- | :---- | :---- |
| Многошаговые бизнес-процессы(заказ, платеж, onboarding) | Естественная декомпозиция на этапы со своими контрактами. Каждый этап тестируется изолированно. Замена одного блока не затрагивает остальные | Отлично |
| Real-time системы(аукционы, чаты, трейдинг) | Событийная модель нативно соответствует WebSocket. ws sid автоматически в ctx. reply\_to\_sender для персональных ответов | Отлично |
| Blockchain/DeFi операции | Изоляция RPC-вызовов, условная маршрутизация по сетям, fan-out на финале, timeout per handler для разных сервисов | Отлично |
| Fan-out реакции | Добавление нового side-effect без изменения существующего кода. Отказоустойчивость: падение одного handler не блокирует остальные | Отлично |
| Worker / Scheduled tasks | Fire-and-forget цепочки, scheduler из коробки, метрики и event log для мониторинга автономных процессов | Отлично |
| Интеграционные слои | Внешний API изолирован в handler с собственным timeout и retry. Единый error-паттерн для всех интеграций | Хорошо |
| ETL пайплайны | Единый state через стадии. Status-based error handling. Финальный report показывает статистику каждого этапа | Хорошо |
| Системы с audit trail | trace\_id сквозной, steps в struct, event\_log встроен, метрики per-handler. Не нужна отдельная инфраструктура | Хорошо |

## **Избыточность фреймворка**

| Сценарий | Обоснование |
| :---- | :---- |
| Простые CRUD без бизнес-логики | Принять запрос, записать в БД, вернуть ответ. Событийный слой не добавляет ценности. Достаточно прямого вызова |
| Одношаговые трансформации | Нет цепочки, нет смысла в шине. Обычная async функция проще |
| Административные скрипты | Миграции, one-off импорт. Событийная архитектура решает несуществующую проблему |
| Статические API (health, metadata) | Ответы без обработки. Overhead без пользы. В khorsyio такие endpoint-ы делаются напрямую через router.get без шины |

## **Принципиальные ограничения**

| Сценарий | Обоснование |
| :---- | :---- |
| Распределенные системы | Шина работает in-process (asyncio.Queue). Для inter-service коммуникации нужны Kafka, RabbitMQ, NATS. Khorsyio решает задачу внутри одного процесса |
| CPU-intensive вычисления | ML inference, crypto mining, рендеринг. Узкое место \- процессор, не организация кода. Событийная шина добавляет overhead без пользы |
| Микросервисы с сетевым транспортом | Фреймворк не обеспечивает транспорт между сервисами. Каждый сервис может использовать khorsyio внутри себя, но связь между сервисами \- за рамками |

# **6\. Мониторинг и наблюдаемость**

Встроенные инструменты без дополнительных зависимостей.

| Инструмент | Что предоставляет |
| :---- | :---- |
| bus.metrics.snapshot() | Per-handler: processed (количество), errors (ошибки), avg\_ms (среднее время), last\_error (последняя ошибка) |
| bus.event\_log.recent(N) | Последние N событий в ring buffer. Фильтрация по event\_type и/или trace\_id. Размер буфера настраивается |
| trace\_id | Сквозной идентификатор. Прокидывается автоматически через forward. Позволяет восстановить весь путь запроса |
| ctx.source | Источник запроса (http, ws:sid, scheduler) доступен в каждом handler |
| Endpoint /metrics | JSON с метриками всех handlers. Подключается одной строкой |
| Endpoint /events | JSON с event log. Поддерживает query параметры n, type, trace |

\# endpoint метрик

app.router.get('/metrics', lambda req, send: Response.json(send, app.bus.metrics.snapshot()))

\# просмотр событий конкретного запроса

app.bus.event\_log.recent(20, trace\_id='abc123')

# **7\. Graceful shutdown**

При остановке приложения шина выполняет контролируемое завершение.

| Этап | Действие |
| :---- | :---- |
| 1\. Stop accepting | Флаг \_running=False, новые события не принимаются |
| 2\. Cancel scheduled | Все scheduled tasks отменяются через task.cancel() |
| 3\. Drain queue | Оставшиеся события в очереди обрабатываются в течение drain\_timeout (default 5s) |
| 4\. Resolve waiters | Pending bus.request получают error envelope с code='shutdown' |
| 5\. Close resources | Закрытие http client, database pool, websocket connections |

# **8\. Контрольный список перед разработкой блока**

Каждый handler должен пройти этот список до написания кода.

| Пункт | Что определить |
| :---- | :---- |
| Входная структура | msgspec.Struct с типизированными полями и дефолтами. Это контракт с предыдущим блоком |
| Выходная структура | msgspec.Struct. Контракт с следующим блоком. При pipeline паттерне \- тот же struct |
| subscribes\_to | Событие на которое подписан. При namespace Domain автоматически добавит префикс |
| publishes | Событие которое публикует. Пустая строка при ручном управлении (условная маршрутизация) или терминальном блоке (fan-out receiver) |
| Зависимости | Какие компоненты нужны: db, client, bus, transport. Определяются через имя параметра в \_\_init\_\_ |
| timeout | Максимальное время обработки. Адекватно задаче: расчет 1-5s, внешний API 15-30s, мониторинг tx до 120s |
| Стратегия ошибок | Status в struct (мягкий, данные проходят дальше) или exception (жесткий, цепочка прерывается) |
| Namespace | Убедиться что handler добавлен в Domain.handlers и namespace корректен |

